# NPL

Mini-proyecto de procesamiento de texto utilizando t√©cnicas cl√°sicas

# Integrantes

Jonathan Pacheco

Joshua Triana

Julio Morales

# Proyecto Final crear tu propio ChatGPT usando un RAG

# üì∞ Retrieval-Augmented Generation (RAG) con Ollama y Langchain

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/jopachecoc/NPL/blob/main/PF_ChatGPT_RAG.ipynb)

---

## üìù Descripci√≥n del Proyecto

Este proyecto implementa un sistema de **Generaci√≥n Aumentada por Recuperaci√≥n (RAG)** utilizando el framework **LangChain** y el motor de modelos de lenguaje local **Ollama**.

El objetivo es crear un sistema capaz de responder preguntas de manera precisa bas√°ndose exclusivamente en un **corpus de documentos externo** (en este caso, documentos de **Wikihow**), en lugar de la informaci√≥n con la que fue pre-entrenado el modelo. Esto permite generar respuestas m√°s espec√≠ficas, actualizadas y evitar alucinaciones.

### üõ†Ô∏è Tecnolog√≠as Utilizadas

- **Ollama:** Motor para ejecutar modelos de lenguaje de c√≥digo abierto de forma local.
- **LangChain:** Framework para el desarrollo de aplicaciones impulsadas por modelos de lenguaje.
- **Vector Stores (ChromaDB/FAISS):** Para almacenar y buscar incrustaciones vectoriales de los documentos.
- **Modelos de Embeddings:** Para convertir el texto en vectores num√©ricos.
- **Corpus:** Documentos de **Wikihow** en espa√±ol.

---

## üöÄ Instalaci√≥n y Uso

### 1. Requisitos Previos

- **GOOLE COOLAB:** Acceder a la plataforma y contar con usuario de google

* Maquina Local: Antes de ejecutar el notebook, aseg√∫rate de tener instalado:

- **Ollama:** Debe estar ejecut√°ndose en tu m√°quina local o servidor.
- **Docker** (Opcional, para un entorno m√°s controlado).

### 2. Ejecuci√≥n

1.  **Clonar el repositorio** (Si aplica) o **Abrir el Notebook en Colab**.
2.  **Instalar dependencias:** Ejecuta la celda de instalaci√≥n de librer√≠as (`pip install ...`).
3.  **Configurar Ollama:** Aseg√∫rate de que el modelo de lenguaje (ej. `llama2`, `mistral`, etc., seg√∫n el notebook) est√© descargado y accesible por el sistema.
4.  **Ejecutar celdas:** Sigue el flujo del notebook:
    - Carga de documentos de Wikihow.
    - Creaci√≥n de `TextSplitter` para dividir el texto.
    - Generaci√≥n y almacenamiento de embeddings en la Vector Store.
    - Configuraci√≥n del **Chain de LangChain** (ej. `RetrievalQA`).
    - Realizaci√≥n de consultas de prueba.

---

## üìä Resultados y Evidencia

Las siguientes im√°genes demuestran la correcta ejecuci√≥n y el funcionamiento del sistema RAG, mostrando la configuraci√≥n de las cadenas, la recuperaci√≥n de documentos y la respuesta final generada por el modelo.

![Configuraci√≥n de la cadena LangChain y muestra de la recuperaci√≥n de documentos relevantes para una consulta.]

### Ejecuci√≥n de Consulta y Respuesta Generada

![Ejemplo de una consulta de prueba, mostrando el prompt final enviado al LLM y la respuesta precisa basada en los documentos de Wikihow.](IMG1ChB2.jpg)

### Vista Detallada de la Respuesta (Evidencia Adicional)

![Evidencia detallada de una respuesta generada por el modelo, confirmando la aplicaci√≥n de la informaci√≥n del contexto recuperado.](IMG1ChB3.jpg) (IMG1ChB1.jpg)

---

---

# ENTREGA_3: Clasificaci√≥n de Texto en Espa√±ol con BERT y Hugging Face

En esta entrega se implementa un clasificador de texto en espa√±ol utilizando modelos pre-entrenados tipo BERT y la librer√≠a Hugging Face Transformers. El objetivo es especializar un modelo BERT en tareas de clasificaci√≥n de sentimientos y noticias, aprovechando transfer learning y fine-tuning.

## Flujo de trabajo

1. **Carga y an√°lisis exploratorio de datasets**

   - Uso de datasets de Hugging Face como `pysentimiento/spanish-tweets` y `tweet_eval`.
   - Estad√≠sticas de longitud y distribuci√≥n de clases.

2. **Preprocesamiento y tokenizaci√≥n**

   - Uso del tokenizador oficial del modelo BERT en espa√±ol ([dccuchile/bert-base-spanish-wwm-cased](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased)).
   - Conversi√≥n de textos a secuencias de tokens y m√°scaras de atenci√≥n.

3. **Preparaci√≥n de datos**

   - Divisi√≥n en conjuntos de entrenamiento, validaci√≥n y prueba.
   - Conversi√≥n de etiquetas a IDs.

4. **Definici√≥n y ajuste del modelo**

   - Uso de `AutoModelForSequenceClassification` de Hugging Face.
   - Entrenamiento como featurizer (congelando BERT) y fine-tuning (entrenando todo el modelo).
   - Experimentaci√≥n con diferentes cabezas de clasificaci√≥n.

5. **Entrenamiento y evaluaci√≥n**
   - Entrenamiento con la clase `Trainer` de Hugging Face.
   - Monitoreo con TensorBoard.
   - Evaluaci√≥n de precisi√≥n y an√°lisis de errores.

## Resultados

- Precisi√≥n superior al 90% en el conjunto de prueba usando fine-tuning.
- El uso de modelos pre-entrenados reduce el tiempo de entrenamiento y mejora la calidad respecto a modelos desde cero.

## Datasets utilizados

- pysentimiento/spanish-tweets'

## Ejecuci√≥n

Puedes ejecutar el notebook [tercera_entrega.ipynb](tercera_entrega.ipynb) localmente o en Google Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jopachecoc/NPL/blob/main/tercera_entrega.ipynb)

# ENTREGA 2: Clasificaci√≥n de Noticias en Espa√±ol con LSTM

Este proyecto implementa un clasificador de noticias en espa√±ol utilizando una red neuronal LSTM (Long Short-Term Memory) con PyTorch Lightning. El objetivo es categorizar titulares de noticias en cinco categor√≠as: Deportes, Salud, Tecnolog√≠a, Colombia y Econom√≠a.

## Descripci√≥n

Se utiliza un dataset de noticias reales del portal RCN, disponible en [Hugging Face Hub](https://huggingface.co/datasets/Nicky0007/titulos_noticias_rcn_clasificadas). El flujo de trabajo incluye:

1. **Carga y an√°lisis exploratorio del dataset**

   - Estad√≠sticas de longitud de los textos.
   - Distribuci√≥n de clases.

2. **Preprocesamiento y tokenizaci√≥n**

   - Tokenizador simple y construcci√≥n de vocabulario.
   - Conversi√≥n de textos a secuencias de tokens.

3. **Preparaci√≥n de datos**

   - Divisi√≥n en conjuntos de entrenamiento, validaci√≥n y prueba.
   - Balanceo de clases.

4. **Definici√≥n del modelo**

   - Implementaci√≥n de un bloque LSTM y una capa densa para clasificaci√≥n.
   - Entrenamiento con PyTorch Lightning.

5. **Entrenamiento y evaluaci√≥n**

   - Ajuste de hiperpar√°metros.
   - Monitoreo con TensorBoard.
   - Evaluaci√≥n de precisi√≥n global y por clase.

6. **Resultados**
   - Precisi√≥n general superior al 90%.
   - An√°lisis de errores y recomendaciones para mejorar la clase "Colombia".

## Requisitos

- Python 3.10+
- PyTorch
- PyTorch Lightning
- datasets
- matplotlib
- scikit-learn
- numpy
- pandas

Instala las dependencias con:

```sh
pip install -r requerimientos.txt
```

## Ejecuci√≥n

Puedes ejecutar el notebook [segundo_entrega.ipynb](segundo_entrega.ipynb) localmente o en Google Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion2/2-nlp-with-lstm.ipynb)

## Notas

- El modelo utiliza una longitud m√°xima de 35 tokens por texto, optimizada seg√∫n la distribuci√≥n del dataset.
- Se emplea el estado oculto final de la LSTM como representaci√≥n del texto.
- El c√≥digo est√° preparado para facilitar la experimentaci√≥n y el ajuste de hiperpar√°metros.

## Resultados

- Precisi√≥n global: ~90%
- Precisi√≥n por clase: superior al 91% en la mayor√≠a de categor√≠as, con margen de mejora en "Colombia".

## Aplicaciones

- Clasificaci√≥n autom√°tica de noticias en portales de contenido.
- Organizaci√≥n y personalizaci√≥n de la experiencia del usuario.

---
